
<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>
    VPGC
  </title>

  <link rel="icon" href="../images/favicon.ico">

  <!-- bootstrap -->
  <script src="https://code.jquery.com/jquery-3.4.1.slim.min.js" integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/css/bootstrap.min.css" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
  <!-- icon -->
  <script src="https://kit.fontawesome.com/87dc3e863a.js" crossorigin="anonymous"></script>
  <!-- font -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
  <style>
    body {
      background: rgb(255, 255, 255) no-repeat fixed top left;
      font-family:'Open Sans', sans-serif;
    }
  </style>

</head>

<body>
  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-0" style="padding-bottom: 0px;">
      <div class="container-fluid">
        <div class="row">
          <div class="col">
            <!-- paper title -->
            <h2 style="font-size:30px;">
              Efficient Video Portrait Reenactment via Grid-based Codebook
            </h2>
            
            <!-- publication -->
            <!-- <h4 style="color:#6e6e6e;"> ArXiv 2023 </h4> -->

            <hr>
            
            <!-- authors -->
            <h6> <a href="https://unibruce.github.io/" target="_blank">Kaisiyuan Wang</a><sup>1</sup>,
                 <a href="https://hangz-nju-cuhk.github.io/" target="_blank">Hang Zhou</a><sup>2</sup>,
                 <a href="https://wuqianyi.top/" target="_blank">Qianyi Wu</a><sup>3</sup>,
                 <a href="https://me.kiui.moe/" target="_blank">Jiaxiang Tang</a><sup>4</sup>,
                 Zhiliang Xu<sup>2</sup>,
                 Borong Liang<sup>2</sup>,
                 Tianshu Hu<sup>2</sup>,
                 Errui Ding<sup>2</sup>,
                 Jingtuo Liu<sup>2</sup>, 
                 <a href="https://liuziwei7.github.io/" target="_blank">Ziwei Liu</a><sup>5</sup>,
                 <a href="https://jingdongwang2017.github.io/" target="_blank">Jingdong Wang</a><sup>2</sup>
                 <br>
                 <br>
            </h6>
            <p> <sup>1</sup> The University of Sydney &nbsp;
                <sup>2</sup> Baidu Inc. &nbsp;
                <sup>3</sup> Monash University &nbsp;
                <sup>4</sup> Peking University &nbsp;
                <sup>5</sup> S-Lab, Nanyang Technological University &nbsp;
                <br>
            </p>
            
            <!-- links -->
            <div class="row justify-content-center">
              <!-- link to paper -->
              <div style="margin: 5px;">
                <p class="mb-5"><a class="btn btn-large btn-dark" href="https://dl.acm.org/doi/pdf/10.1145/3588432.3591509" role="button" target="_blank">
                <i class="fa fa-file"></i> 
                Paper 
                </a> </p>
              </div>
              <!-- link to video -->
              <!-- <div style="margin: 5px;">
                <p class="mb-5"><a class="btn btn-large btn-dark disabled" href="TODO" role="button" target="_blank">
                <i class="fa-brands fa-youtube"></i> 
                Video
                </a> </p>
              </div> -->
              <!-- link to code -->
              <div style="margin: 5px;">
                <p class="mb-5"><a class="btn btn-large btn-dark disabled" href="https://github.com/uniBruce/VPGC_Pytorch" role="button" target="_blank">
                <i class="fa fa-github-alt"></i> 
                Code 
                </a> </p>
              </div>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- video carousel -->
  <!-- <div> 
    <div class="col-12 text-center">
      <h3>Demonstration</h3>
      <hr style="margin-top:0px">
    </div>
    <div class="row justify-content-center" style="align-items:center; display:flex; margin-bottom: 20px;">
      <video width="18%" controls style="margin: 5px;"> <source src="videos/obama_intro.mp4" type="video/mp4"> </video>
      <video width="18%" controls style="margin: 5px;"> <source src="videos/murphy_intro.mp4" type="video/mp4"> </video>
      <video width="18%" controls style="margin: 5px;"> <source src="videos/engm_intro.mp4" type="video/mp4"> </video>
      <video width="18%" controls style="margin: 5px;"> <source src="videos/marco_intro.mp4" type="video/mp4"> </video>
      <video width="18%" controls style="margin: 5px;"> <source src="videos/chris_intro.mp4" type="video/mp4"> </video>
    </div>
    <div class="col-12 text-center">
      (Videos synthesized by our method, and audio synthesized by <a href="https://speechify.com/text-to-speech-online/">Speechify</a> from text.)
    </div>
  </div> -->
  <br>

  <!-- abstract -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Abstract</h3>
          <hr style="margin-top:0px">
        </div>
          <p class="text-justify">
            While progress has been made in the field of portrait reenactment, the problem of how to efficiently produce high-fidelity and accu- rate videos remains. Recent studies build direct mappings between driving signals and their predictions, leading to failure cases when synthesizing background textures and detailed local motions. In this paper, we propose the Video Portrait via Grid-based Codebook (VPGC) framework, which achieves efficient and high-fidelity por- trait modeling. Our key insight is to query driving signals in a position-aware textural codebook with an explicit grid structure. The grid-based codebook stores delicate textural information lo- cally according to our observations on video portraits, which can be learned efficiently and precisely. We subsequently design a Prior- Guided Driving Module to predict reliable features from the driving signals, which can be later decoded back to high-quality video por- traits by querying the codebook. Comprehensive experiments are conducted to validate the effectiveness of our approach.
          </p>
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- method -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Method</h3>
          <hr style="margin-top:0px">
          <!-- text -->
          </div>
            <p class="text-justify">
              Our key insight is to learn a personalized grid-based codebook that can facilitite efficient and high-fidelity portrait modeling.
            </p>
          </div>
          <!-- image -->
          <div class="row justify-content-center" style="align-items:center; display:flex;">
            <div> 
              <img src="images/pipeline.png" alt="pipeline" class="img-responsive" width="100%"/>
            </div>
          </div>
      </div>
    </div>
  </section>
  <br>

  <!-- comparison results -->


  <!-- Visual results -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Supplementary Video</h3>
            <hr style="margin-top:0px">
            <!-- text -->
            </div>
              <p class="text-justify">
                Our demo video includes self-reenactment and cross-reenactment comparison as well as ablation study examples.
              </p>
            </div>
            <!-- video -->
            <div class="section demo">
                <br>
                <center>
                  <iframe width="640" height="480" src="images/384_copyright_compress.mp4" frameborder="0" allowfullscreen></iframe>
                </video>
                    </center>
                    </div>
            <!-- <div id="carouselExampleIndicators" class="carousel slide" data-ride="carousel">
              <ol class="carousel-indicators">
                <li data-target="#carouselExampleIndicators" data-slide-to="0" class="active"></li>
                <li data-target="#carouselExampleIndicators" data-slide-to="1"></li>
              </ol>
              <div class="carousel-inner">
                <div class="carousel-item active">
                  <video class="d-block w-100" autoplay loop controls>
                    <source src="images/384_copyright_compress.mp4" type="video/mp4">
                  </video>
                </div>
                <!-- <div class="carousel-item">
                  <video class="d-block w-100" autoplay loop controls>
                    <source src="videos/cnw.mp4" type="video/mp4">
                  </video>
                </div> -->
              </div> -->
            </div>
        </div>
      </div>
    </div>
  </section>
  <br>  


  <!-- citing -->
  <div class="container">
    <div class="row ">
      <div class="col-12">
        <h3>Citation</h3>
        <hr style="margin-top:0px">
        <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
<code>@inproceedings{10.1145/3588432.3591509,
    author = {Wang, Kaisiyuan and Zhou, Hang and Wu, Qianyi and Tang, Jiaxiang and Xu, Zhiliang and Liang, Borong and Hu, Tianshu and Ding, Errui and Liu, Jingtuo and Liu, Ziwei and Wang, Jingdong},
    title = {Efficient Video Portrait Reenactment via Grid-Based Codebook},
    year = {2023},
    isbn = {9798400701597},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3588432.3591509},
    doi = {10.1145/3588432.3591509},
    abstract = {While progress has been made in the field of portrait reenactment, the problem of how to efficiently produce high-fidelity and accurate videos remains. Recent studies build direct mappings between driving signals and their predictions, leading to failure cases when synthesizing background textures and detailed local motions. In this paper, we propose the Video Portrait via Grid-based Codebook (VPGC) framework, which achieves efficient and high-fidelity portrait modeling. Our key insight is to query driving signals in a position-aware textural codebook with an explicit grid structure. The grid-based codebook stores delicate textural information locally according to our observations on video portraits, which can be learned efficiently and precisely. We subsequently design a Prior-Guided Driving Module to predict reliable features from the driving signals, which can be later decoded back to high-quality video portraits by querying the codebook. Comprehensive experiments are conducted to validate the effectiveness of our approach.},
    booktitle = {ACM SIGGRAPH 2023 Conference Proceedings},
    articleno = {66},
    numpages = {9},
    keywords = {Facial Animation, Video Synthesis},
    location = {Los Angeles, CA, USA},
    series = {SIGGRAPH '23}
}</code></pre>
      </div>
    </div>
  </div>

  <footer class="text-center" style="margin-bottom:10px; font-size: medium;">
      <hr>
      Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the <a href="https://lioryariv.github.io/idr/" target="_blank">website template</a>.
  </footer>

</body>
</html>

